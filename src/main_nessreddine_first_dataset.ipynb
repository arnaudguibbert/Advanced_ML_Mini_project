{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from data_processing import *\n",
    "from neural_net import *\n",
    "from time import perf_counter\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "data = pd.read_csv(\"../Data/Postures.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing first line and last sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[1:].drop(columns=['X11', 'Y11', 'Z11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing rows with empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace('?', np.NaN).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using users with less than 300 rows as testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_users_for_testing = [i for i in data.User.unique() if len(data[data['User'] == i]) < 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_users_for_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data[data['User'].isin(data_users_for_testing)].drop(columns=[\"User\"])\n",
    "data = data[~data['User'].isin(data_users_for_testing)].drop(columns=[\"User\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.Class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.Class.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only be training using the classes: {1, 2, 5} and testing for classes {2, 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_data['Class']\n",
    "labels = data['Class']\n",
    "data = data.drop(columns=[\"Class\"])\n",
    "test_data = test_data.drop(columns=[\"Class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, validation_data, train_labels, validation_labels) = train_test_split(data, labels, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct classification using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the values of k for our k-Nearest Neighbor classifier along with the\n",
    "# list of accuracies for each value of k\n",
    "kVals = range(1, 30, 2)\n",
    "accuracies = []\n",
    "\n",
    "# loop over various values of `k` for the k-Nearest Neighbor classifier\n",
    "for k in range(1, 30, 2):\n",
    "    # train the k-Nearest Neighbor classifier with the current value of `k`\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    # evaluate the model and update the accuracies list\n",
    "    score = model.score(validation_data, validation_labels)\n",
    "    print(\"k=%d, accuracy=%.2f%%\" % (k, score * 100))\n",
    "    accuracies.append(score)\n",
    "\n",
    "# find the value of k that has the largest accuracy\n",
    "i = int(np.argmax(accuracies))\n",
    "print(\"k=%d achieved highest accuracy of %.2f%% on validation data\" % (kVals[i], accuracies[i] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-train our classifier using the best k value and predict the labels of the\n",
    "# test data\n",
    "model = KNeighborsClassifier(n_neighbors=kVals[i])\n",
    "model.fit(data, labels)\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# show a final classification report demonstrating the accuracy of the classifier\n",
    "# for each of the digits\n",
    "print(\"EVALUATION ON TESTING DATA\")\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using KNN after dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction using LLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 200\n",
    "n_components = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components, method='standard')\n",
    "data_lle = clf.fit_transform(data)\n",
    "train_data_lle = clf.fit_transform(train_data)\n",
    "validation_data_lle = clf.fit_transform(validation_data)\n",
    "test_data_lle = clf.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_lle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_lle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the values of k for our k-Nearest Neighbor classifier along with the\n",
    "# list of accuracies for each value of k\n",
    "kVals = range(250, 300, 2)\n",
    "accuracies = []\n",
    "\n",
    "# loop over various values of `k` for the k-Nearest Neighbor classifier\n",
    "for k in range(250, 300, 2):\n",
    "    # train the k-Nearest Neighbor classifier with the current value of `k`\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(train_data_lle, train_labels)\n",
    "\n",
    "    # evaluate the model and update the accuracies list\n",
    "    score = model.score(validation_data_lle, validation_labels)\n",
    "    print(\"k=%d, accuracy=%.2f%%\" % (k, score * 100))\n",
    "    accuracies.append(score)\n",
    "\n",
    "# find the value of k that has the largest accuracy\n",
    "i = int(np.argmax(accuracies))\n",
    "print(\"k=%d achieved highest accuracy of %.2f%% on validation data\" % (kVals[i], accuracies[i] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-train our classifier using the best k value and predict the labels of the\n",
    "# test data\n",
    "model = KNeighborsClassifier(n_neighbors=kVals[i])\n",
    "model.fit(data_lle, labels)\n",
    "predictions = model.predict(test_data_lle)\n",
    "\n",
    "# show a final classification report demonstrating the accuracy of the classifier\n",
    "# for each of the digits\n",
    "print(\"EVALUATION ON TESTING DATA\")\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the DeepSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def extract_image_set(x_data: np.array, y_data :np.array, agg_fun=np.sum, n_images=3) -> Tuple[np.array, np.array]:\n",
    "  \"\"\"\n",
    "  Extract a single set of images with corresponding target\n",
    "  :param x_data\n",
    "  \"\"\"\n",
    "  idxs = np.random.randint(low=0, high=len(x_data)-1, size=n_images)\n",
    "  return x_data[idxs], agg_fun(y_data[idxs])\n",
    "\n",
    "\n",
    "def generate_dataset(n_samples: int, x_data: np.array, y_data :np.array, agg_fun=np.sum, n_images=3) -> Tuple[List[List[np.array]], np.array]:\n",
    "  \"\"\"\n",
    "  :return X,y in format suitable for training/prediction \n",
    "  \"\"\"\n",
    "  generated_list = [extract_image_set(x_data, y_data, agg_fun, n_images) for i in tqdm(range(n_samples))]\n",
    "  X, y = [i[0] for i in generated_list], np.array([t[1] for t in generated_list])\n",
    "  output_lists = [[] for i in range(n_images)]\n",
    "  for image_idx in range(n_images):\n",
    "    for sample_idx in range(n_samples):\n",
    "      output_lists[image_idx].append(np.expand_dims(X[sample_idx][image_idx], axis=2))\n",
    "  return output_lists, y\n",
    "\n",
    "X_train_data, y_train_data = generate_dataset(n_samples=100000, x_data=x_train, y_data=y_train, n_images=3)\n",
    "X_test_data, y_test_data = generate_dataset(n_samples=20000, x_data=x_test, y_data=y_test, n_images=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "n_sample_viz, n_images = 3, 3\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_sample_viz, ncols=n_images, figsize=(9.0, 9.0))\n",
    "\n",
    "for sample_idx in range(n_sample_viz):\n",
    "  for im_idx in range(n_images):\n",
    "    axes[sample_idx, im_idx].imshow(X_train_data[im_idx][sample_idx][:, :, 0], cmap='Greys')\n",
    "    axes[sample_idx, im_idx].axis('off')\n",
    "    if im_idx==0:\n",
    "      axes[sample_idx, 0].set_title('               Sum value for this row is {}'.format(y_train_data[sample_idx]), \n",
    "                                    fontsize=15, loc='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os, datetime\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "filters = 64\n",
    "kernel_size = 3\n",
    "\n",
    "# Shared portion of the network\n",
    "input_image = Input(shape=input_shape)\n",
    "\n",
    "y = Conv2D(32, kernel_size=(3, 3),\n",
    "           activation='relu',\n",
    "           input_shape=input_shape)(input_image)\n",
    "y = Conv2D(64, (3, 3), activation='relu')(y)\n",
    "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
    "y = Dropout(0.25)(y)\n",
    "y = Flatten()(y)\n",
    "output_vec = Dense(128, activation='relu')(y)\n",
    "\n",
    "feature_ext_model = Model(input_image, output_vec)\n",
    "\n",
    "image_1, image_2, image_3 = Input(shape=input_shape), Input(shape=input_shape), \\\n",
    "                            Input(shape=input_shape)\n",
    "outputs_1, outputs_2, outputs_3 = feature_ext_model(image_1), feature_ext_model(image_2), feature_ext_model(image_3)\n",
    "\n",
    "y = Add()([outputs_1, outputs_2, outputs_3])\n",
    "y = Dense(64, activation='relu')(y)\n",
    "y = Dense(32, activation='relu')(y)\n",
    "encoded = Dense(1)(y)\n",
    "\n",
    "model = Model([image_1, image_2, image_3], encoded)\n",
    "adam = Adam()\n",
    "model.compile(optimizer=adam, loss='mae')\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=X_train_data, y=y_train_data,\n",
    "          epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(x=X_test_data)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "n_sample_viz, n_images = 4, 3\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_sample_viz, ncols=n_images, figsize=(12.0, 12.0))\n",
    "for sample_idx in range(n_sample_viz):\n",
    "  for im_idx in range(n_images):\n",
    "    axes[sample_idx, im_idx].imshow(X_test_data[im_idx][sample_idx][:, :, 0], cmap='Greys')\n",
    "    axes[sample_idx, im_idx].axis('off')\n",
    "    if im_idx==0:\n",
    "      axes[sample_idx, 0].set_title('Sum value for this row is {}, prediction is {}'.format(y_test_data[sample_idx], round(prediction[sample_idx][0])), \n",
    "                                    fontsize=15, loc='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
