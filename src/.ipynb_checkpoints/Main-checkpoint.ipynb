{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import LocallyLinearEmbedding as LLE\n",
    "from data_processing import normalize, split_data, generate_pairplot, find_hyper, KNN_classifier\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "Classes = [\"Eyeglasses\",\"Wearing_Hat\",\"Wavy_Hair\",\"Goatee\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-president",
   "metadata": {},
   "source": [
    "# Visualize the transformation applied on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "path = [\"../Data/selected_images/000720.jpg\",\n",
    "        \"../Data/selected_images/201763.jpg\",\n",
    "        \"../Data/selected_images/189506.jpg\",\n",
    "        \"../Data/selected_images/120827.jpg\"]\n",
    "\n",
    "\n",
    "operator = nn.AvgPool2d(2, stride=2)\n",
    "img_tensor = torch.empty(4,1,218,178)\n",
    "img_color_tensor = torch.empty(4,218,178,3)\n",
    "\n",
    "for i,path_im in enumerate(path):\n",
    "    \n",
    "    image = cv2.cvtColor(cv2.imread(path_im),cv2.COLOR_BGR2RGB).astype(float)/255 # RGB image\n",
    "\n",
    "    gray = cv2.cvtColor(cv2.imread(path_im), cv2.COLOR_BGR2GRAY).astype(float)/255 # gray scale image\n",
    "    assert(gray.shape == (218,178))\n",
    "    \n",
    "    img_color_tensor[i] = torch.from_numpy(image)\n",
    "    img_tensor[i,0,:,:] = torch.from_numpy(gray)\n",
    "    \n",
    "\n",
    "operator = nn.AvgPool2d(2, stride=2)\n",
    "reduc_img_tensor = operator(img_tensor)\n",
    "\n",
    "figure = plt.figure(figsize=[25,8])\n",
    "\n",
    "for i,path in enumerate(path):\n",
    "    ax1 = figure.add_subplot(2,6,3*i+1)\n",
    "    ax2 = figure.add_subplot(2,6,3*i+2)\n",
    "    ax3 = figure.add_subplot(2,6,3*i+3)\n",
    "    ax1.imshow(img_color_tensor[i])\n",
    "    ax2.imshow(img_tensor[i,0],cmap='Greys_r')\n",
    "    ax3.imshow(reduc_img_tensor[i,0],cmap='Greys_r')\n",
    "    ax1.set_title(\"Original image Class \" + str(i))\n",
    "    ax2.set_title(\"Gray scale image Class \" + str(i))\n",
    "    ax3.set_title(\"Averaged image Class \" + str(i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-affiliate",
   "metadata": {},
   "source": [
    "# Import the dataset (data_frame.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/data_frame.csv\")\n",
    "data.head(10) # Let's have a look at the sructure of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np = data.iloc[:,1:].to_numpy().astype(float)\n",
    "data_np = data_np[data_np[:,0] != 2]\n",
    "img = data.iloc[5].to_numpy() # Take a random image\n",
    "img = img[2:].astype(float).reshape(109,89)\n",
    "plt.imshow(img,'Greys_r') # Check if the preprocessing has been well done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-dimension",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-purchase",
   "metadata": {},
   "source": [
    "We will test and compare two dimensionality reduction methods : Locally Linear Embeddings (LLE) and Modified Locally Linear Embeddings (MLLE). To do so we will see if this these algorithms are able to separate the 4 classes in four different clusters (idest to make easier the classification task). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = [0.2,0]\n",
    "train_set, validation_set, test_set = split_data(data_np,ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train set shape :\",train_set.shape)\n",
    "#print(\"validation set shape :\",validation_set.shape)\n",
    "print(\"test set shape :\",test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_bis, _, test_set_bis = split_data(train_set,[0.7,0])\n",
    "classifier = KNN_classifier(train_set_bis[:,1:],train_set_bis[:,0])\n",
    "classifier.score(test_set_bis[:,1:],test_set_bis[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_components = np.arange(4,50,4)\n",
    "range_neighbors = np.arange(10,50,5)\n",
    "x,y,metric,KNN_metric = find_hyper(train_set,range_components,range_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[14,5])\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "cs1 = ax1.contourf(x,y,metric)\n",
    "cs2 = ax2.contourf(x,y,KNN_metric,cmap=\"viridis\")\n",
    "ax1.set_xlabel(\"Number of components\")\n",
    "ax1.set_ylabel(\"Number of neighbors\")\n",
    "ax2.set_xlabel(\"Number of components\")\n",
    "ax2.set_ylabel(\"Number of neighbors\")\n",
    "ax1.set_title(\"Reconstruction error\")\n",
    "ax2.set_title(\"KNN accuracy\")\n",
    "plt.colorbar(cs1,ax=ax1)\n",
    "plt.colorbar(cs2,ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = LLE(n_components=10,n_neighbors=25,method=\"modified\")\n",
    "X_trans = embedding.fit_transform(data_np[:,1:])\n",
    "X_reduc = np.concatenate((data_np[:,[0]],X_trans),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,10])\n",
    "generate_pairplot(X_reduc,Classes,components_to_show=4)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
