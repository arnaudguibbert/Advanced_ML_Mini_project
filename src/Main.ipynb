{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.manifold import LocallyLinearEmbedding as LLE\n",
    "from utils import normalize, Assessment, plot_time_comparison\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-alpha",
   "metadata": {},
   "source": [
    "# About the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-reason",
   "metadata": {},
   "source": [
    "The data set contains some information about mails (frequency of some words, special characters, number of capital letters etc...). Each mail is then classified as spam (class = 1) or not spam (class = 0). Each datapoint has 56 features, if you want to get more information about these features, you can go in the data folder where all these information are detailed (or you can go directly on the website where the data set is available https://archive.ics.uci.edu/ml/datasets/Spambase). The goal of this project is to find a manifold where the data of interest lies (if it exists one). Here, the data of interest is of course to know if a mail is a spam or not. Let's then try to find a lower-dimensional space where we can easily separate these two classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Class\"]\n",
    "columns += [\"Frequence word \" + str(i) for i in range(1,49)]\n",
    "columns += [\"Special character \" + str(i) for i in range(1,7)]\n",
    "columns += [\"Capital length mean\"]\n",
    "columns += [\"Capital length longest\"]\n",
    "columns += [\"Sum of captital length\"]\n",
    "data = pd.read_csv(\"../Data/spambasedata.csv\",names=columns)\n",
    "data.shape # Let's have a look at the sructure of the data set\n",
    "data_np = data.to_numpy().astype(float) # Convert it to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = LLE(random_state=10)\n",
    "embed.fit(data_np)\n",
    "print(embed.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dist = data.groupby([\"Class\"]).count().iloc[:,0].reset_index()\n",
    "class_dist.rename(columns={\"Frequence word 1\":\"Number of data points\"},inplace=True)\n",
    "sns.barplot(data=class_dist,x=\"Class\",y=\"Number of data points\")\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-dimension",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-purchase",
   "metadata": {},
   "source": [
    "We will test and compare two dimensionality reduction methods : Locally Linear Embeddings (LLE) and one of its derivative Modified Locally Linear Embeddings (MLLE). To do so we will see if this these algorithms are able to extract the relevant information, idest to separate the 2 classes in two different clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-ridge",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_components = np.arange(2,51,4)\n",
    "range_neighbors = np.arange(10,41,5)\n",
    "myalgo = Assessment(data_np,range_components,range_neighbors,check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = myalgo.crossksets(data_np)\n",
    "print(\"number of sets\",len(train_set))\n",
    "print(\"size of one training set\",train_set[1].shape)\n",
    "print(\"size of one testing set\",test_set[1].shape)\n",
    "print(\"size of last training set\",train_set[-1].shape)\n",
    "print(\"size of last testing set\",test_set[-1].shape)\n",
    "print(\"ratio of classes\",train_set[0][train_set[0][:,0] == 1].shape[0]/train_set[0].shape[0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_2, test_set_2 = myalgo.crossksets(data_np)\n",
    "print(\"Number of different values :\",(train_set[0] != train_set_2[0]).sum())\n",
    "print(\"Number of different values :\",(train_set[1] != train_set_2[1]).sum())\n",
    "print(\"Number of different values :\",(test_set[0] != test_set_2[0]).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-rapid",
   "metadata": {},
   "source": [
    "# Locally Linear Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_components = np.arange(1,4,2)\n",
    "range_neighbors = np.arange(10,101,10)\n",
    "LLE_algo = Assessment(data_np,range_components,range_neighbors,k=5,check=False,run=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-replication",
   "metadata": {},
   "source": [
    "### Have a look on the first 4 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLE_algo.generate_pairplot(700,4,save_file=\"LLE_pairplot\",title=\"teub\",norm_0100=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLE_algo.generate_3Dplot(10,\"LLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-confusion",
   "metadata": {},
   "source": [
    "## Hyperparameters analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-movie",
   "metadata": {},
   "source": [
    "1 classical metric and two additional ones have been chosen to determine the number of neighbors to use:\n",
    "* The reconstruction error, or the cumulative sum of the eigenvalues\n",
    "* The accuracy and the F1 measure using linear SVM to separate the classes\n",
    "* The accuracy and the F1 measure using KNN to separate the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLE_algo.reset()\n",
    "LLE_algo.find_hyper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-protest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LLE_algo.generate_all(save_file=\"LLE_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLE_algo.plot_cumulative_error(title=\"Reconstruction error\",save_file=\"LLE_cum_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-poetry",
   "metadata": {},
   "source": [
    "# Modified Locally Linear Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_components = np.arange(2,8,5)\n",
    "range_neighbors = np.arange(70,101,10)\n",
    "MLLE_algo = Assessment(data_np,range_components,range_neighbors,k=5,method=\"modified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-debate",
   "metadata": {},
   "source": [
    "### Have a look on the first 4 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLLE_algo.generate_pairplot(100,20,title=\"yeah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,8])\n",
    "MLLE_algo.generate_3Dplot(50,fig,[1,1,1],\"MLLE\")\n",
    "fig.savefig(\"test.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-tulsa",
   "metadata": {},
   "source": [
    "## Hyperparameters analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-fifteen",
   "metadata": {},
   "source": [
    "1 classical metric and two additional ones have been chosen to determine the number of neighbors to use:\n",
    "* The reconstruction error, or the cumulative sum of the eigenvalues\n",
    "* The accuracy and the F1 measure using linear SVM to separate the classes\n",
    "* The accuracy and the F1 measure using KNN to separate the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLLE_algo.reset()\n",
    "MLLE_algo.find_hyper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLLE_algo.generate_all(save_file=\"not none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLLE_algo.plot_cumulative_error(title=\"Reconstruction error\",save_file=\"MLLE_cum_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
